{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chapter 8. Applying Machine Learning to Sentiment Analysis\n",
    "https://www.amazon.co.jp/Python-Machine-Learning-Cutting-edge-Predictive/dp/1783555130\n",
    "\n",
    "この章では、文章の内容が肯定的か否定的かを分類する評判分析(Sentiment analysis)という、自然言語処理の一分野を説明している。以下の流れで説明していく。自然言語特有の内容は2, 3項目の、文章の整形と特徴ベクトルの生成で、他は一般的な機械学習の内容である。\n",
    "\n",
    "1. 入力データをファイルから読み込み\n",
    "1. 入力データとなるレビュー文章の整形\n",
    "1. レビュー文章から特徴ベクトルの生成\n",
    "1. 機械学習アルゴリズムを用いたレビュー文章の分類学習\n",
    "1. Out-of-Core学習を使用した大規模データの処理\n",
    "\n",
    "まずは利用するPython, packageのバージョンを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python (>=3.4.3) : 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)]\n",
      "numpy (>=1.9.1): 1.12.0\n",
      "pandas (>=0.15.2): 0.19.2\n",
      "sklearn (>=0.15.2): 0.18.1\n",
      "nltk: 3.2.2\n"
     ]
    }
   ],
   "source": [
    "from sys import version\n",
    "print(\"Python (>=3.4.3) :\", version)\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy (>=1.9.1):\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas (>=0.15.2):\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn (>=0.15.2):\", sklearn.__version__)\n",
    "\n",
    "# scipy required >=0.14.0\n",
    "\n",
    "import nltk\n",
    "print(\"nltk:\", nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Obtaining the IMDb Movie Review Dataset\n",
    "入力データとして、スタンフォード大学の下記URLで公開されている50000件の映画のレビュー文章(Internet Movie Database, IMDb)を使用する。\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "上記URLにある aclImdb_v1.tar.gz をダウンロードし7-zip等で解凍するとaclImdbフォルダに下図のような構成でファイルが展開される。\n",
    "\n",
    "<pre>\n",
    "aclImdb\n",
    "├test\n",
    "│├neg: 12500件の否定的レビュー文章ファイルが格納されている。\n",
    "│├pos: 12500件の肯定的レビュー文章ファイルが格納されている。\n",
    "│└others\n",
    "│\n",
    "├train\n",
    "│├neg: 12500件の否定的レビュー文章ファイルが格納されている。\n",
    "│├pos: 12500件の肯定的レビュー文章ファイルが格納されている。\n",
    "│└others\n",
    "│\n",
    "├README\n",
    "└others\n",
    "</pre>\n",
    "\n",
    "各negフォルダには12500件の否定的レビュー文章が、各posフォルダには12500件の肯定的レビュー文章が1件ずつテキストファイルとして保存されている。例えば \"Widow hires a psychopath as a handyman. Sloppy film noir thriller which doesn't make much of its tension promising set-up. (3/10)\"\n",
    "という文章が1つのファイルに保存されている。\n",
    "\n",
    "ファイル名のフォーマットは index_score.txt で、indexは各レビューの識別番号で0-12499の整数、scoreは各レビュースコアで1-10の整数である。1-4を否定的レビュー、7-10を肯定的レビューと定義しているので、scoreに入る数字はnegフォルダのファイルには1,2,3,4のいずれか、posフォルダのファイルには7,8,9,10のいずれかになる。\n",
    "\n",
    "まずはレビュー文章をpandaのDataFrameに読み込む。review columnにレビュー文章を、sentiment columnに二値化したレビュースコア、(否定的なら0、肯定的なら1)を格納する。本章では、計算時間を短くするために、pos, neg, 各フォルダに125件のみ保存し合計500件のレビューのみ使用する。NELやSPAの文字コードが文章にあるファイルは読み込みエラーが生じたので文章から該当文字コードのみ削除して再保存してある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame()\n",
    "label_dic = {'pos':1, 'neg':0}\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = './Ch_8_aclImdb/' + s + '/' + l\n",
    "        for f in os.listdir(path):\n",
    "            with open(path+'/'+f, 'r') as infile:\n",
    "                review_txt = infile.read()\n",
    "            df = df.append([[review_txt, label_dic[l]]], ignore_index=True)\n",
    "df.columns = ['review', 'sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The finest short I've ever seen. Some commenta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a very, very odd film...one that is so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although Bullet In The Brain is, without quest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...means \"take up and read\", which is precisel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  The finest short I've ever seen. Some commenta...          1\n",
       "2  This is a very, very odd film...one that is so...          1\n",
       "3  Although Bullet In The Brain is, without quest...          1\n",
       "4  ...means \"take up and read\", which is precisel...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>..Oh wait, I can! This movie is not for the ty...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>I have not seen many low budget films i must a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>I received this movie as a gift, I knew from t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment\n",
       "495  ..Oh wait, I can! This movie is not for the ty...          0\n",
       "496  I have not seen many low budget films i must a...          0\n",
       "497  I received this movie as a gift, I knew from t...          0\n",
       "498  Some films that you pick up for a pound turn o...          0\n",
       "499  This is one of the dumbest films, I've ever se...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Bag-of-words Model\n",
    "本章では、第4章 Preprocessingでカテゴリーデータを数値データに変換したのと同じ考え方で、bag-of-wordsと呼ばれる変換手法を導入する。bag-of-wordsは以下の2ステップで構成する。\n",
    "\n",
    "1. 入力文章データから、トークンの辞書を作成する。トークンとは1単語や2単語等で表す言葉の単位のようなもので、例えば \"word\", \"good example\" 等である。\n",
    "1. 入力文章データに含まれる各トークンの数で特徴ベクトルを構成する。例えば入力文章が \"This is a pen. It is good.\" で、辞書が [\"is\", \"pen\", \"example\"] ならば、特徴ベクトルは [2, 1, 0] となる。特徴ベクトルの次元は、辞書のトークンの数となる。\n",
    "\n",
    "入力文章データは辞書全体の一部のトークンのみ使用するため特徴ベクトルの多くの要素はゼロとなり疎となるため、スパース(Sparse)性があるなどと呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Words into Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "    ])\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書を確認する。keyがトークン、itemが特徴ベクトルでの要素番号を表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'is': 1, 'shining': 2, 'sun': 3, 'sweet': 4, 'the': 5, 'weather': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴ベクトルをnumpy arrayへ変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のトークンは1単語のみであるため1-gram or unigram modelと呼ばれる。他に連続する2単語以上をトークンに含めることも可能で2-gram, 3-gram,,, などと呼ぶ。例えば \"The sun\" は2-gramのトークンで、\"weather is shining\" は3-gramのトークンである。実装するにはCountVectorizerのインスタンスを生成する際にngram_rangeオプションを指定する。例えば1-gramから3-gramを辞書に加えたい場合は ngram_range=(1,3) とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Word Relevancy via Term Frequency-inverse Document Frequency\n",
    "自然言語処理をする際に多くの文章データに同じ単語が含まれている場合がある。この単語は有益な情報を含まない可能性が高い。この考えを実装し特徴ベクトルの該当要素の重要度を下げる、term frequency-inverse document frequency (tf-idf)という操作をする。scikit-learnでのtf-idfの定義を以下に示す。\n",
    "$$tfidf(t, d) = tf(t,d) \\times (1+idf(t, d))$$\n",
    "$$idf(t, d) = \\log\\frac{1+n_d}{1+df(d, t)}$$\n",
    "ここで、$t$はトークン、$d$は文章データ、$n_d$は入力する文章データの総数を表す。$tf$はterm frequencyの略で、文章$d$の中にあるトークン$t$の数を返す。$df$はdocument frequencyの略で、トークン$t$を含む文章$d$の数を返す。\n",
    "\n",
    "\"1+\"はゼロ割を防ぎ過剰な重み付けをしないための処置であり、この\"1+\"の有無でtf-idfの流派が存在する。\n",
    "\n",
    "scikit-learnではTfidfTransformerクラスに実装されている。注意事項としては2点あり、入力は文章データそのものではなく特徴ベクトルであることと、返り値はデフォルトでL2ノルムで正規化されていることである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.43,  0.56,  0.56,  0.  ,  0.43,  0.  ],\n",
       "       [ 0.  ,  0.43,  0.  ,  0.  ,  0.56,  0.43,  0.56],\n",
       "       [ 0.4 ,  0.48,  0.31,  0.31,  0.31,  0.48,  0.31]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform(bag).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text Data\n",
    "前節の特徴ベクトル化の前に入力文章を整形する必要がある場合が多い。本章では以下の処理をする。\n",
    "\n",
    "* htmlマークアップの除去\n",
    "* non-word文字の除去。ただし顔文字は残す。\n",
    "\n",
    "これらの処理を実装するために正規表現モジュールregexを使用する。より詳細にはparserの使用が適している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preproc(text):\n",
    "    # htmlマークアップの除去\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    # non-word文字の除去。ただし顔文字は残す\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # 文章中にある顔文字を格納\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())                    # 文章を全て小文字にしnon-word文字を半角スペースに変換\n",
    "    text += ' '.join(emoticons).replace('-', '')                 # 格納していた顔文字を最後に追加\n",
    "    return text\n",
    "\n",
    "# サンプル\n",
    "text = \"</a>This :) is :( a test :-)!\"\n",
    "preproc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力レビュー文章に適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.review = df.review.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Documents into Tokens\n",
    "文章を整形後は文章を半角スペースで単語ごとに分割しリスト化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# サンプル\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更に、runs, runningなどのrunの変形を原型のrunに置換する、ステミングという処理をする。今回は、Natural Language Toolkit for Python (NLTK)というモジュールに実装されているPorter stemmerアルゴリズムを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# サンプル\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'thus'が'thu'になってしまっているので副作用もある模様。この問題にはレンマ化という処理をする。ただし計算コストが高い。\n",
    "\n",
    "その他のstemmingアルゴリズムとしてSnowball stemmerやLancaster stemmerが挙げられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その他のトピックにstop-word removalという処理がある。stop-wordとはisやandなど、どの文章にも含まれていそうな単語を表し、bag-of-wordsによる特徴ベクトルから無条件で除去する操作をstop-word removalという。この処理もNLTKモジュールを利用して実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\takashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression Model for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Bigger Data - Online Algorithms and Out-of-core Learning\n",
    "Latent Dirichelet allocation\n",
    "\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
